In this chapter, we provide an evaluation proof of using IRS in kernel space. 
Experiments are setup for understanding the behavior of the six prototypes proposed in this thesis. 
Additionally, they are also used to provide a comparison with the user space implementation of IRS. 

\section{Setup}

The evaluation is performed with a virtual machine running on a hardware with maximum of 16 cores. 
The virtual machine can use from two to eight processor cores which is used for the scaled up evaluation. 
The virtual machine is based on Intel Xeon E5-2650 - 2.00 GHz configured with Ubuntu 17.04 as the operating system. 
It is configured with 4GB RAM and 80GB hard disk. 
The virtual machine is configured with the LLVM-CLANG 3.9, GCC 4.9 and Boost 1.6.2.

\section*{Note}

We would be using the following abbreviations in the rest of the evaluations for simplicity of expression.
\begin{itemize}
\item {IRS\_Shared} - IRS user space implementation using shared scheduler design
\item {IRS\_Opt} - IRS user space implementation using additional scheduler with conditional variable design.
\item {Proto\_1} - Prototype 1 discussed in the previous chapter.
\item {Proto\_2} - Prototype 2 discussed in the previous chapter.
\item {Proto\_3} - Prototype 3 discussed in the previous chapter.
\item {Proto\_4} - Prototype 4 discussed in the previous chapter.
\item {Proto\_5} - Prototype 5 discussed in the previous chapter.
\item {Proto\_6} - Prototype 6 discussed in the previous chapter.

\end{itemize}

\section{Evaluation Metrics}

\subsection{Execution Overhead}

Evaluation is done between the IRS user space solution vs kernel space solutions. 
Execution overhead is calculated for each solution with respect to the plain execution of the bench-marking program. 
Unconstrained execution of program is considered as plain execution. 

$$Overhead = (T_{sol} - T_{plain})/T_{plain} * 100$$

$T_{sol}$ is the execution of the bench-marking program when executed with the scheduling constraints. 
$T_{plain}$ is the plain execution of the bench-marking program. 

\subsection{Number of valid synchronization calls} 

It is used to realize the number of voluntary calls made to kernel space for synchronization purposes. 
It is primarily the number of IOCTL calls under the command - context\_switch, signal\_all\_other\_threads or set\_clock.


\section{Benchmarks}

We use four different bench-marking programs for the evaluation of this thesis. 
The bench-marking programs include:
\begin{itemize}
\item{Fibonacci} - Program runs with two threads computing Fibonacci numbers for 25 iterations per thread.
\item{Last Zero} - Program runs with 16 threads~\citep{abdulla2014optimal}.
\item{Indexer}- Program runs with 15 threads~\citep{dynamic_por}.
\item{Dining Philosophers Problem} - Program runs with 16 threads. These threads are classified as odd and even philosophers thread. 
In this benchmark, only one class of philosopher thread is active at any point of time. 
This benchmark is motivated from the solution presented in \citet{silberschatz2014operating}.
\end{itemize}

\section{Voluntary kernel level calls}

We evaluate the number of voluntary calls made to kernel space for synchronization. 
The evaluation is done across all six prototypes. 
The benchmark used for the evaluation is Fibonacci. 
The Fibonacci benchmark presents different levels of memory constraints via its traces. 
It has three traces providing 98 constraints, 44 constraints and 24 constraints respectively.

\begin{table}
\begin{center}
 \begin{tabular}{|c c c|} 
 \hline
 & Prototype 1-4 & Prototype 5-6\\ %[0.5ex] 
 \hline
 Trace-1 & 300 & 175\\ 
 Trace-2 & 300 & 150\\
 Trace-3 & 300 & 150\\
 \hline
\end{tabular}
\end{center}
\caption{Number of IOCTL calls}
\label{num_ioctls}
\end{table}
\begin{table}
\begin{center}
 \begin{tabular}{|c c c|} 
 \hline
 & Prototype 1-4 & Prototype 5-6\\ %[0.5ex] 
 \hline
 Trace-1 & 150 & 27\\ 
 Trace-2 & 150 & 0\\
 Trace-3 & 150 & 0\\
 \hline
\end{tabular}
\end{center}
\caption{Number of context switch calls}
\label{num_ctxts}
\end{table}

From the tables~\ref{num_ioctls} and \ref{num_ctxts}, it is really evident that prototypes 5 and 6 reduce the number of calls made to kernel space. 
Prototypes 5 \& 6 are expected to provide better performance compared other prototypes when there are less dependencies between threads even with high number of memory events. 
Let us consider the Fibonacci benchmark, it has two threads with a total of 75 shared-memory events per thread. 
Thus, making a total of 150 memory events. 
For every memory event, prototypes 1-4 trigger IOCTL calls to kernel space for context\_switch, signal\_all\_other\_threads or set\_clock. 
Therefore, having number of IOCTL calls as 300. 
In case of prototype 5-6, we have a proxy checking in user space which drastically reduces the calls to kernel space for additional synchronization. 
The set\_clock ioctl command is the only call made consistently for every memory access when using prototypes 5-6.

\begin{table}[h]
\begin{center}
 \begin{tabular}{|c c c c c c c|} 
 \hline
 & Proto-1 & Proto-2 & Proto-3 & Proto-4 & Proto-5 & Proto-6\\ %[0.5ex] 
 \hline
 Trace-1 & 406.833 & 454.785 & 385.416 & 455.745 & 277.793 & 275.343 \\ 
 Trace-2 & 367.199 & 520.352 & 352.506 & 509.843 & 160.266 & 160.307 \\
 Trace-3 & 351.029 & 416.653 & 333.704 & 412.206 & 152.425 & 153.06\\
 \hline
\end{tabular}
\end{center}
\caption{Execution overhead(\%) when compared with plain execution of Fibonacci}
\label{fib_exec_over}
\end{table}

Table~\ref{fib_exec_over} presents the reasoning of using prototypes 5-6. 
It shows the execution overhead is drastically reduced for the above mentioned prototypes. 
Reduction in the number of IOCTL calls makes a huge difference in the execution overhead. 
Prototypes 5-6 performs the best, when the following condition holds:
$num\_memory\_constraints << total\_memory\_events$.

\section{Scaled-up Evaluation}

In this evaluation, we understand the merits and demerits in the performance of the six prototypes and the two user space IRS implementations. 
For this evaluation, we use three bench-marking programs - last zero, indexer and dining philosophers problem. 
We scale the core count from two to eight processor cores and monitor the changes the performance overhead across the three benchmarks for various implementations. 

\subsection{Last Zero}

\citet{abdulla2014optimal} showcases this benchmark for the evaluation of their dynamic POR. 
The last zero program has 16 threads at its disposal. 
The pseudo code of this benchmark is depicted in Fig~\ref{code_lastzero}. 
\begin{figure}[h]
\begin{lstlisting}[mathescape=true,style=customc]
Variables: int arr[0...N] := {0,0..,0}, i;
Thread 0: for (i:=N; array[i]!=0; i--);
Thread j($j \in 1..N$): arr[j] := arr[j-1] + 1;
\end{lstlisting}
\caption{Last Zero Program}
\label{code_lastzero}
\end{figure}
This benchmark is meant to provide memory constraints spread across different threads rather than being in two threads. 
The number of memory constraints provided in the trace files include: 15,12,5,1 respectively. 
The maximum number of possible memory events is 46.

\subsubsection{Two Cores}

The benchmark has 16 threads and the memory constraints are spread across threads. 
Thus, making it expensive in regard to execution time. 
IRS\_Shared is expected to be the worst of all designs considering the fact that it is a busy waiting design. 
Busy waiting design makes the waiting thread to constantly poll one of the cores thus, making it performance inefficient. 
IRS\_Opt is expected to provide a better performance among all the designs because the $num\_memory\_constraints << total\_memory\_events$. 
Proto\_5 and Proto\_6 are also expected to provide better results with this benchmark, since the above condition holds good for these prototypes.

\subsection{Indexer}

\begin{figure}[h]
\begin{lstlisting}[style=customc]
Thread-global (shared) variables:
const int size = 128;
const int max = 4;
int[size] table;

Thread-local variables:
int m = 0, w, h;

Code For thread tid:
while (true) {
	w := getmsg();
	h := hash(w);
	while (cas(table[h],0,w) == false) {
		h := (h+1) % size;
	}
}
int getmsg() {
	if (m < max ) {
		return (++m) * 11 + tid;
	} else {
		exit(); // terminate
	}
}
int hash(int w) {
	return (w * 7) % size;
}
\end{lstlisting}
\label{code_indexer}
\caption{Indexer Program}
\end{figure}


\subsection{Dining Philosopher's Problem}

\begin{figure}[h]
\begin{lstlisting}[style=customc]
Thread-global (shared) variables:
const int size = THREAD_COUNT;
const int num_iter = N;
int[size] chopsticks;

Thread-local variables:
int i;

Code For thread id:
for(i=0; i<num_iter; i++) {
	while((chopsticks[id%THREAD_COUNT]!=0) && \\
	 (chopsticks[(id-1)%THREAD_COUNT]!=0);
	if(id%2==0) {
		chopsticks[id%THREAD_COUNT] = 1;
		chopsticks[(id-1)%THREAD_COUNT] = 1;
	}
	else {
		chopsticks[(id-1)%THREAD_COUNT] = 1;
		chopsticks[id%THREAD_COUNT] = 1;
	}
	chopsticks[id%THREAD_COUNT] = 0;
	chopsticks[(id-1)%THREAD_COUNT] = 0;
}
\end{lstlisting}
\label{code_dining_phil}
\caption{Dining Philosopher's Problem Program}
\end{figure}