The previous chapter highlighted the need for debugging tools or methodologies for solving concurrency bugs in multithreaded programming. 
This thesis is conceived with a solution based on iterative relaxed scheduling(IRS). 
However, there are other techniques which address various solutions using deterministic multithreading(DMT). 
In this chapter, we explore various DMT based solutions and the similarities they share with IRS. 

\section{DTHREADS}

\citet{dthreads} presents a deterministic multithreading runtime system. 
It is build on top of PThreads(POSIX Thread) library in Linux. 
The dthreads library replaces most of the pthread library functions with its own implementation enforcing determinism in execution of threads. 
All the threads created using dthreads library are created as processes and they use a deterministic memory commit protocol for synchronizing the conceived shared memory state. 
The idea of using ``threads-as-processes'' paradigm is motivated from the work of \citet{grace}. 
By moving the design to processes, dthreads eliminates false sharing and provides protection faults. 
For simplicity we would call these threads as dthreads. 
``Twining and diffing'' technique is used to perform the deterministic  memory commit protocol in dthreads. 
Dthreads are run independently until they reach a synchronization point where the diffing step of the above technique takes place. 
It compares the modification in the memory page with the twin page with contains the shared state. 
Each dthread enters the differential comparison(diffing) step based on token being passed around the dthreads. 
Dthreads consists of two phases of execution: parallel and serial phases. 
The twining-diffing step occurs in the serial phase of execution. 
Transitions to serial phase occurs statically. 
Any synchronization operation will result in a transition to serial phase. 

Dthreads creates private, per-process copies of modified pages between commits. 
This would increase the program's memory footprint by the number of modified pages between synchronization operations. 
In case of IRS, we have do not change the implementation of pthreads. 
Therefore, the above problem never occurs in IRS however, there is a possibility of false sharing. 
IRS implementation emphasizes on memory level granularity whereas, dthreads focuses on the synchronization operations encountered in the user program. 
IRS follows a recorder-replay model whereas, dthreads uses ``Twining and diffing'' of threads. 
In IRS, we have a verifier which records the execution traces that are deemed to be safe considering the correctness of the program execution. 
It also consists of a scheduler which can be considered as the replay part of the model. 
In this thesis, we migrate the scheduler module to the kernel space from user space. 
The scheduler would run the instrumented user program with the execution traces generated by the verifier. 
Dthreads does not have a verifier or a separate scheduler module instead, it has library level support to enforce the determinism in execution. 
Also it does not instrument the user program. 
Dthreads is C/C++ library support implemented entirely in user space whereas, in this thesis we highlight the IRS scheduler implemented in kernel space. 

\section{GRACE}

\citet{grace} presents a deterministic library support in Grace. 
The design is similar to the Dthreads implementation. 
Grace also uses ``threads-as-processes'' paradigm. 
However, it is primarily targeted at fork-join models. 
Grace focuses on multithreaded designs which highlight thread creation and joining. 
The reason for the inclusion is that it also falls under the category of DMT based solutions.
However, it has lot drawbacks - it focuses on fork-join models only. 
It is similar to dthreads in a lot of regard.  
IRS is completely different to the Grace implementation.

\section{PEREGRINE}

\citet{peregrine} conceives an alternative DMT solution with schedule relaxation in PEREGRINE runtime system. 
It is a record-replay based implementation. 
It combines two different scheduling designs - sync schedule and memory schedules. 
The hybrid scheduling design is enforce efficiency and determinism in the execution of multithreaded program. 
Unlike the previously mentioned DMT solutions, PEREGRINE uses an instrumentor in LLVM and a user space scheduler for the replay of execution trace. 
PEREGRINE executes the multithreaded program with a certain input to generate its execution trace. 
The recorder records the trace for the given input of the program. 
Replayer/scheduler reuses the same execution trace for the given input of the program. 
It enforces the execution trace on the user program for same input thus, providing a level of determinism in its execution. 
It shares a lot of similarities with IRS. 
IRS is also record-replay based design. 
Both these designs provide memory level granularity. 
However, IRS design generates more traces with less memory level constraints for iteration therefore, retaining some level of non-determinism in the execution of the program. 
The memory level determinism in IRS is enforced based on the order of the memory access. Whereas, in case of PEREGRINE it is enforced based on the output of the program. 
PEREGRINE uses the same execution trace for different inputs to the multithreaded program. 
Whereas, IRS improves the degree of non-determinism in the execution of multithreaded program with every iteration of verifier.

\section{KENDO}

KENDO is another DMT solution proposed by \citet{kendo}, which uses modified Linux kernel to support deterministic logical time. 
KENDO is a software framework, which enforces weak deterministic execution of general purpose lock-based C/C++ based multithreaded programs.  
Weak determinism ensures a deterministic order of all lock acquisitions for a given program input.   
KENDO is a subset of pthreads library. 
It achieves determinism with the use of deterministic logical time, which is used to track the progress of each thread in a deterministic manner. 
KENDO has a kernel level implementation to enforce deterministic execution of threads. 
The IRS implementation highlighted in this thesis focuses on a scheduler implemented in kernel space for enforcing the memory constraints provided in the execution traces by the verifier. 
KENDO does not have any instrumentation of user program unlike PEREGRINE or IRS. 
KENDO only focuses on determinism in lock acquisitions and not on all shared memory accesses whereas, IRS addresses memory-level granularity for all shared memory accesses in the multithreaded program.

\section{COREDET}

\citet{coredet} came up with a compiler and runtime system enforcing deterministic multithreaded execution in COREDET. 
It is another runtime implementation based on DMT. 
COREDET has two phases - parallel and serial phases similar to the Dthreads. 
It has an instrumentor tool in LLVM for instrumenting memory events similar to PEREGRINE. 
COREDET is one of the first DMT solution which addressed the shared memory events and provided memory level granularity. 
COREDET can be executed in two different ways - ownership tracking and store buffering. 
First approach tracks the ownership of data and serializes the execution whenever threads communicate. Such an approach yields sequentially consistent executions and lower overheads, but lower scalability. 
Second approach uses memory versioning without any form of speculation and relaxes memory ordering, yielding higher scalability at the cost of higher overheads.  
It shares a lot of similarities with IRS implementation. 
Both the solutions use LLVM for instrumenting memory events in the multithreaded program. 
However, COREDET uses a round-robin scheduling when it enters a serial phase of execution. 
Whereas in case of IRS on occurrence of a memory event, the scheduler checks for the memory access permission for the given thread with the recorded trace. 
COREDET does not have any record-replay implementation. 
It can be conceived as a runtime implementation with emphasis on fine grained memory access with serialized commits.  


\section{PARROT}

PARROT is another runtime solution based on DMT from \citet{parrot}. 
Compared to other DMT solutions which maps one schedule for one input as depicted in figure~\ref{determinisitic_mapping}, PARROT proposes an approach which uses stable multithreading(StableMT). 
In StableMT, we reuse each schedule on a wide range of inputs, mapping all inputs to a dramatically reduced set of schedules. 
PARROT is a pthread compactible implementation. 
PARROT provides weak determinism similar to KENDO but offers stability. 
PARROT can be integrated with DBUG\citep{dbug} - open source model checker in Linux for determining bugs in the schedules. 
\citet{parrot} shows us that PARROT-DBUG ecosystem is more effective than either system alone. 
DBUG checks the schedules that matter to PARROT and the developers. 
Whereas, PARROT reduces the number of schedules to be checked by DBUG thus, increasing the coverage of DBUG. 
Compared to IRS, PARROT does not have any static code analysis done inside the multithreaded programs. 
The determinism provided by PARROT is relative to three factors: external input, performance critical sections, data races with respect to the enforced synchronization schedules. 
IRS focuses on memory level granularity whereas, PARROT is focused on weak determinism similar to KENDO. 
PARROT-DBUG focuses on StableMT with exhaustive testing of all schedules whereas in IRS, the execution of a multithreaded program can be initiated with a single execution trace from the verifier. 
IRS generates a new trace for every iteration whereas in case of PARROT-DBUG, the execution is blocked until the DBUG checks all the schedules.

\section*{Inference}

From the above sections, it is abundantly clear that there not any solutions which come close to the IRS. 
COREDET is the only implementation which seems to provide memory level granularity similar to IRS. 
PEREGRINE is another implementation which depicts a record-replay paradigm similar to IRS. 
PARROT-DBUG presents a StableMT focused on checking a set of reduced schedules for all the provided inputs in the multithreaded program. 
Other solutions presented in this chapter focus on DMT solutions aimed at synchronization operations rather than memory level accesses. 

